{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic Regression is an example of <b>supervised learning</b> and <b>binary classification problem</b>.\n",
    "* <b>Supervised Learning:</b> input dataset composed by input and labels $(x, y)$ and want to learn a mapping $x \\longrightarrow y$\n",
    "* <b>Classification:</b> the value $y$ to predict is discrete \n",
    "* <b>Binary classification</b> problem $y \\in \\{0, 1\\}$\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "We want the hypothesis outputs value between 0 and 1 ( $h_\\theta(x) \\in [0,1] $ ).<br>\n",
    "Then, for the logistic regression, the hypothesis is:\n",
    "\n",
    "$$ h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^T x}} $$\n",
    "\n",
    "## Logistic Function or Sigmoid Function\n",
    "The function $g(z)$ is called logistic function or sigmoid function and outputs values between 0 and 1. <br>\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "The derivates of the sigmoid function is:\n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(z) & = \\frac{d}{dz} \\frac{1}{1+e^{-z}} \\\\\n",
    "      & = \\frac{d}{dz} (1+e^{-z})^{-1} \\\\\n",
    "      & = \\frac{1}{(1+e^{-z})^2}(e^{-z}) \\\\\n",
    "      & = \\frac{1}{(1+e^{-z})} \\cdot \\bigg(1 - \\frac{1}{(1+e^{-z})} \\bigg) \\\\\n",
    "      & = g(z)(1-g(z))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Assuming that:\n",
    "$$ P(y=1| x;\\theta) = h_\\theta(x)$$\n",
    "$$ P(y=0| x;\\theta) = 1 - h_\\theta(x)$$\n",
    "\n",
    "The two equation can be combined in one equation as follows: \n",
    "\n",
    "$$ P(y| x;\\theta) = h_\\theta(x)^y (1 - h_\\theta(x))^{1-y} $$\n",
    "\n",
    "* if $y=1 \\longrightarrow P(y| x;\\theta) = h_\\theta(x) $\n",
    "* if $y=0 \\longrightarrow P(y| x;\\theta) = 1-h_\\theta(x) $\n",
    "\n",
    "\n",
    "Likelihood:\n",
    "$$\n",
    "\\begin{align*} \n",
    " \\mathcal{L}(\\theta) & = P(\\vec{y}/x; \\theta) \\\\\n",
    "                     & = \\prod_{i=1}^{m} P(y^{(i)}/x^{(i)}; \\theta) \\\\\n",
    "                     & = \\prod_{i=1}^{m} h_\\theta(x^{(i)})^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{(1-y^{(i)})}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Log Likelihood:\n",
    "\\begin{align*}\n",
    "                \\ell(\\theta) & = log \\mathcal{L}(\\theta) \\\\\n",
    "                             & = log \\prod_{i=1}^{m} h_\\theta(x^{(i)})^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{(1-y^{(i)})} \\\\\n",
    "                             & = \\sum_{i=1}^{m} y^{(i)} log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1 - h_\\theta(x^{(i)}))\n",
    "\\end{align*}\n",
    "\n",
    "<b>Maximum Likelihood Estimation</b>: choose $\\theta$ to <b>maximize</b> the <b>log likelihood</b> $\\ell(\\theta)$.\n",
    "\n",
    "## Batch Gradient Ascent\n",
    "\n",
    "To <b>maximize</b> the <b>likelihood</b> we can use <b>gradient ascent</b>:\n",
    "\n",
    "$$ \\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial\\theta_j} \\ell(\\theta) \\:\\:\\: \\forall j $$\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Firstly, compute the partial derivate for one training examples:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial\\theta_j} \\ell(\\theta) & = \\frac{\\partial}{\\partial\\theta_j} \\bigg(y \\ log \\ h_\\theta(x) + (1-y) \\ log \\ (1 - h_\\theta(x)) \\bigg) \\\\\n",
    "                                                   & = \\frac{\\partial}{\\partial\\theta_j} \\bigg(y \\ log \\ g(\\theta^Tx) + (1-y) \\ log \\ (1 - g(\\theta^Tx)) \\bigg) \\\\\n",
    "                                                   & = y \\frac{1}{g(\\theta^Tx)} g(\\theta^Tx) (1 - g(\\theta^Tx)) x_j - (1-y) \\frac{1}{(1-g(\\theta^Tx))} g(\\theta^Tx) (1-g(\\theta^Tx)) x_j \\\\\n",
    "                                                   & = y(1-g(\\theta^Tx))x_j-(1-y)g(\\theta^Tx)x_j \\\\\n",
    "                                                   & = y(1-h_\\theta(x))x_j-(1-y)h_\\theta(x)x_j \\\\\n",
    "                                                   & = \\bigg(y(1-h_\\theta(x))-(1-y)h_\\theta(x)\\bigg)x_j \\\\\n",
    "                                                   & = \\bigg(y - yh_\\theta(x) - h_\\theta(x) + yh_\\theta(x)\\bigg)x_j \\\\\n",
    "                                                   & = (y - h_\\theta(x))x_j\n",
    "\\end{align*}\n",
    "$$\n",
    "Then, for m training examples:\n",
    "$$ \\frac{\\partial}{\\partial\\theta_j} \\ell(\\theta) = \\sum_{i = 1}^{m} (y^{(i)} - h_\\theta(x^{(i)}))\\ x_j^{(i)} $$\n",
    "$$ \\theta_j := \\theta_j + \\alpha \\sum_{i = 1}^{m} (y^{(i)} - h_\\theta(x^{(i)}))\\ x_j^{(i)} \\:\\:\\: \\forall j $$\n",
    "\n",
    "## Logistic Regression vs Linear Regression\n",
    "\n",
    "The main differences are:\n",
    "\n",
    "1. Update rule: \n",
    "   * $ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\theta_j - \\alpha \\sum_{i = 1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})\\ x_j^{(i)} \\qquad $ Linear Regression (Gradient Descent)\n",
    "   * $ \\theta_j := \\theta_j + \\alpha \\frac{\\partial}{\\partial\\theta_j} \\ell(\\theta) = \\theta_j + \\alpha \\sum_{i = 1}^{m} (y^{(i)} - h_\\theta(x^{(i)}))\\ x_j^{(i)} \\qquad $ Logistic Regression (Gradient Ascent) \n",
    "<br>\n",
    "<br>\n",
    "2. Minimization vs Maximization\n",
    "   * In Linear Regression Minimize the squared errors\n",
    "   * In Logistic Regression Maximize the log likelihood \n",
    "<br>\n",
    "<br>\n",
    "3. Definition of $h_\\theta(x)$\n",
    "\n",
    "They are similar because they belong the same class of algorithms: <b>General Linear Models</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.INIT_PARAMETERS = {\"zero\", \"random\"}\n",
    "        self.theta = None\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, Y, iterations, learning_rate, batch_size=512, init_parameters=\"zero\", step_per_iterations=10):\n",
    "        \n",
    "        # First dimension of X,Y (n_samples) must be the same\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"Error: first dimension of X {} and Y {} not matches.\".format(X.shape[0], Y.shape[0]))\n",
    "\n",
    "        m = X.shape[0]\n",
    "        nx = X.shape[1]\n",
    "        n = nx + 1\n",
    "        \n",
    "        # Add 1 in first component of all Xs (interpect term)\n",
    "        X = np.insert(X, 0, 1.0, axis=1)\n",
    "        Y = np.reshape(Y, (Y.shape[0], 1))\n",
    "        \n",
    "        # Initialize paramters theta\n",
    "        self._init_weights(n, init_parameters)\n",
    "        \n",
    "        history, execution_time = self.mini_batch_gradient_descent(X, Y, m, n, batch_size,\n",
    "                                                                   iterations, learning_rate, step_per_iterations)\n",
    "\n",
    "        print(\"Training takes {:.2f} seconds.\".format(execution_time))\n",
    "        return history\n",
    "    \n",
    "    def mini_batch_gradient_descent(self, X, Y, m, n, batch_size, iterations, learning_rate, step_per_iterations):\n",
    "        start_time = time.time()\n",
    "\n",
    "        history = []\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            J = 0\n",
    "            n_batch = m // batch_size\n",
    "            n_batch_remainder = m % batch_size\n",
    "            \n",
    "            for i_batch in range(n_batch):\n",
    "                X_batch = X[i_batch:i_batch + batch_size]\n",
    "                Y_batch = Y[i_batch:i_batch + batch_size]\n",
    "\n",
    "                # Forward Propagation\n",
    "                J_batch, Diff = self.forward_propagation(X_batch, Y_batch, batch_size)\n",
    "                \n",
    "                J += J_batch\n",
    "\n",
    "                # Backward Propagation\n",
    "                self.backward_propagation(X_batch, Diff, batch_size, learning_rate)\n",
    "                \n",
    "                history.append(J)\n",
    "            \n",
    "            if n_batch_remainder != 0:\n",
    "                X_batch = X[:-n_batch_remainder]\n",
    "                Y_batch = Y[:-n_batch_remainder]\n",
    "                \n",
    "                # Forward Propagation\n",
    "                J_batch, Diff = self.forward_propagation(X_batch, Y_batch, n_batch_remainder)\n",
    "                \n",
    "                J += J_batch\n",
    "\n",
    "                # Backward Propagation\n",
    "                self.backward_propagation(X_batch, Diff, n_batch_remainder, learning_rate)\n",
    "                \n",
    "                history.append(J)\n",
    "\n",
    "            if iteration and iteration % step_per_iterations == 0:\n",
    "                print(\"Iteration {} - Cost {}\".format(iteration, J))\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        return history, execution_time\n",
    "\n",
    "    def forward_propagation(self, X, Y, m):\n",
    "        # Compute the hypothesis for all Xs in the batch\n",
    "        H = self._compute_hypothesis(X)\n",
    "        \n",
    "        # Compute the difference between estimated y and actual label\n",
    "        Diff = Y - H\n",
    "        \n",
    "        # Sum the squared differences\n",
    "        J = (1 / (2 * m)) * np.dot(Diff.T, Diff)\n",
    "        return J, Diff\n",
    "    \n",
    "    \n",
    "    def backward_propagation(self, X, Diff, m, learning_rate):\n",
    "        Grad = (1 / m) * np.dot(X.T, Diff)\n",
    "        \n",
    "        # Update paramters theta\n",
    "        self.theta += learning_rate * Grad\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Add 1 in first component of all Xs (interpect term)\n",
    "        X = _add_intercept(X) \n",
    "        return self._compute_hypothesis(X)\n",
    "    \n",
    "    def _compute_hypothesis(self, X):\n",
    "        Z = np.dot(X,self.theta)\n",
    "        return self.sigmoid(Z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_intercept(X):\n",
    "        # Add intercept term (1.0 in the first component of each input)\n",
    "        return np.insert(X, 0, 1.0, axis=1)\n",
    "    \n",
    "    def _init_weights(self, n, init_parameters=\"zero\"):\n",
    "        \"\"\" initiates the paramters as zero or random. \"\"\"\n",
    "        if init_parameters not in self.INIT_PARAMETERS:\n",
    "            raise ValueError(\"Error: init_parameters must be one of %s.\" % self.INIT_PARAMETERS)\n",
    "\n",
    "        if init_parameters == \"zero\":\n",
    "            # Initialize paramters with zero values\n",
    "            self.theta = np.zeros((n, 1), dtype=float)\n",
    "\n",
    "        if init_parameters == \"random\":\n",
    "            # Initialize paramters with random values\n",
    "            self.theta = np.random.rand(n, 1)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.343250</td>\n",
       "      <td>-1.331148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.820553</td>\n",
       "      <td>-0.634668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.986321</td>\n",
       "      <td>-1.888576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.944373</td>\n",
       "      <td>-1.635452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.976734</td>\n",
       "      <td>-1.353315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2\n",
       "0  1.343250 -1.331148\n",
       "1  1.820553 -0.634668\n",
       "2  0.986321 -1.888576\n",
       "3  1.944373 -1.635452\n",
       "4  0.976734 -1.353315"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_FOLDER = os.path.join(\"..\",\"datasets\",\"logistic-regression-data\")\n",
    "\n",
    "df_x = pd.read_csv(os.path.join(DATASET_FOLDER, \"train_inputs.txt\"), sep=\"\\ +\", names=[\"x1\",\"x2\"], header=None, engine='python')\n",
    "df_y = pd.read_csv(os.path.join(DATASET_FOLDER, \"train_labels.txt\"), sep='\\ +', names=[\"y\"], header=None, engine='python')\n",
    "df_y = df_y.astype(int)\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    y\n",
       "94  1\n",
       "95  1\n",
       "96  1\n",
       "97  1\n",
       "98  1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y[\"y\"] = df_y.apply(lambda row: max(row[\"y\"], 0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_x[[\"x1\", \"x2\"]].values\n",
    "Y = df_y[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 - Cost [[1.66962032]]\n",
      "Iteration 200 - Cost [[1.58336183]]\n",
      "Iteration 300 - Cost [[1.56543842]]\n",
      "Iteration 400 - Cost [[1.56135067]]\n",
      "Iteration 500 - Cost [[1.56056015]]\n",
      "Iteration 600 - Cost [[1.56057329]]\n",
      "Iteration 700 - Cost [[1.56074047]]\n",
      "Iteration 800 - Cost [[1.56089695]]\n",
      "Iteration 900 - Cost [[1.56101187]]\n",
      "Training takes 0.14 seconds.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "init_parameters = \"random\"  # Random or Zero init\n",
    "batch_size = 16\n",
    "\n",
    "history = LR_model.fit(X=X,\n",
    "                       Y=Y,\n",
    "                       iterations=iterations,\n",
    "                       learning_rate=learning_rate,\n",
    "                       init_parameters=init_parameters,\n",
    "                       batch_size=batch_size,\n",
    "                       step_per_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}