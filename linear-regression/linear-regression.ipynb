{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear Regression (also called <b>ordinary least squares</b>) is the simplest example of <b>supervised learning</b> and <b>regression problem</b>.\n",
    "* <b>Supervised Learning:</b> input dataset composed by input and labels $(x, y)$ and want to learn a mapping $x \\longrightarrow y$\n",
    "* <b>Regression:</b> the value $y$ to predict is continuous\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "* $x$: inputs (or features)\n",
    "* $y$: outputs (or targets) \n",
    "* $(x, y)$: one training example \n",
    "* $(x^{(i)},y^{(i)})$: <i>i-th</i> training example \n",
    "* $x^{(i)}_{j}$: <i>j-th</i> feature (or component) of the <i>i-th</i> training example \n",
    "* $m$: number of training examples\n",
    "* $n_{x}$: number of input features\n",
    "* $n = n_{x} + 1$: actual number of input features (adding the dummy feature $x_{0}= 0$)\n",
    "* $\\theta$: parameters\n",
    "* $x_{0}=1$: intercept term (a dummy feature is added)\n",
    "\n",
    "### Hypothesis\n",
    "The <b>hypothesis h</b> is that the outpus is a linear function of the input features (fit a straight line).<br>\n",
    "Then we decide to approximate $y$ as a linear function of $x$:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0x_0 + ... + \\theta_nx_n \n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sum_{j = 1}^{n}\\theta_j x_j = \\theta^{T}x \n",
    "$$\n",
    "\n",
    "### Cost Function\n",
    "The objective of the <b>learning algorithm</b> is to learn (choose) parameters $\\theta$ such that the outputs $h_{\\theta}(x)$ are close to the actual labels ($h_{\\theta}(x) \\approx y$) at least for the training examples.<br>\n",
    "In order to do this, we want to <b>minimize</b> the <b>squared difference between predictions and the actual labels</b> over all $m$ training examples:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta} \\frac{1}{2}\\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "So the <b>loss</b> function is defined as the distance for one training example:\n",
    "\n",
    "$$\n",
    "Loss_{\\theta} (x^{(i)}, y^{(i)}) = \\frac{1}{2}(h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "and the <b>cost</b> function $J(\\theta)$ is defined as the <b>sum</b> of the loss function over the $m$ training examples:\n",
    "\n",
    "$$\n",
    "\\begin{align} J(\\theta) & = \\sum_{i = 1}^{m} Loss_{\\theta} (x^{(i)}, y^{(i)})\\\\ & = \\frac{1}{2}\\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 \\end{align}\n",
    "$$\n",
    "\n",
    "For <b>linear regression</b>, the cost function $J(\\theta)$, is defined as a sum of sqaure terms and then is is a quadratic function. For this reason, it not have local optimum but only the global optimum.  \n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "\n",
    "The algorithm starts with some initial values of $\\theta$ ($\\theta = \\vec{0}$ or random) and keep changing $\\theta$ to reduce $J(\\theta)$. <br>\n",
    "* The <b>direction</b> to which take a little step to go downhill as fast as possible in the cost function (minimize $J(\\theta)$ ) it determined by the direction of the <b>steepest descent</b>.\n",
    "* The <b>size</b> of the change (update) is determined by a parameter $\\alpha$ called <b>learning rate</b>.\n",
    "\n",
    "Then the update rule becomes:\n",
    "$$ \n",
    "\\theta_j \\mathrel{\\mathop:}= \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j} J(\\theta) \\:\\:\\: \\forall j\n",
    "$$ \n",
    "\n",
    "Where $\\theta_{j}$ is the <i>j-th</i> component of the parameters vector, and the update is performed for each component of $\\theta$.\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\frac{\\partial}{\\partial\\theta_j} \\frac{1}{2}\\sum_{i = 1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2  \n",
    "$$\n",
    "\n",
    "Firstly, compute the <b>partial derivate</b> with respect to <b>one training example</b>:\n",
    "\n",
    "$$ \n",
    "\\begin{align} \\frac{\\partial}{\\partial\\theta_j} J(\\theta) \n",
    "& = \\frac{\\partial}{\\partial\\theta_j} \\frac{1}{2}(h_\\theta(x) - y)^2  \\\\ \n",
    "& = 2 \\cdot \\frac{1}{2} (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial\\theta_j} (h_\\theta(x) - y) \\\\ \n",
    "& = (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial\\theta_j} \\ (\\sum_{j = 1}^{n}\\theta_j x_j - y) \\\\ & = (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial\\theta_j} \\ (\\theta_0 x_0 \\ + ... + \\ \\theta_j x_j + ... + \\theta_n x_n - y) \\\\ \n",
    "& = (h_\\theta(x) - y) \\cdot (\\frac{\\partial}{\\partial\\theta_j} \\ \\theta_0 x_0 \\ + ... + \\ \\frac{\\partial}{\\partial\\theta_j} \\ \\theta_j x_j \\ + ... + \\ \\frac{\\partial}{\\partial\\theta_j} \\ \\theta_n x_n - \\frac{\\partial}{\\partial\\theta_j} \\ y) \\\\ \n",
    "& = (h_\\theta(x) - y) \\cdot \\ (0 \\ + ... + \\ 0 \\ + \\ x_j \\ + \\ 0 \\ + ... + \\ 0 \\ - \\ 0)  \\\\ & = (h_\\theta(x) - y)\\ x_j \\end{align} \n",
    "$$\n",
    "\n",
    "To obtain the <b>partial derivate</b> with respect to the <b>cost function</b> $J(\\theta)$ we needs to sum over all training examples (the derivate of sum is the sum of derivate):\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\sum_{i = 1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})\\ x_j^{(i)}\n",
    "$$\n",
    "\n",
    "The final <b>update rule</b> for <b> batch gradient descent becomes </b>: \n",
    "Then the update rule becomes:\n",
    "\n",
    "$$ \n",
    "\\theta_j \\mathrel{\\mathop:}= \\theta_j - \\alpha \\sum_{i = 1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})\\ x_j^{(i)} \\:\\:\\: \\forall j\n",
    "$$ \n",
    "\n",
    "\n",
    "The <b>batch gradient descent</b> algorithm consists of update until convergence the update rule defined above\n",
    "At each iteration of batch gradient descent the algorithm scans through all the training examples (for this the term batch)\n",
    "\n",
    "Repeat { <br>\n",
    "$ \\qquad \\qquad \\theta_j \\mathrel{\\mathop:}= \\theta_j - \\alpha \\sum_{i = 1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})\\ x_j^{(i)} \\:\\:\\: \\forall j$ <br>\n",
    "}\n",
    "\n",
    "The main disadvantage of <b>batch gradient descent</b> is that, in order to make one update to the parameters $\\theta$, is needed to scan the entire training set. Then, every single step of batch gradient descent update becomes very slow because needs to iterate over all the training set.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "In <b>stochastic gradient descent</b>, instead of scan through the entire training set to make one update, it updates the paramters $\\theta$ computing the derivate of just one training example. <br>\n",
    "On average, the algorithm goes nearby the global minimum but never exactly converges (it oscillates near the global minimum). <br>\n",
    "The <b>stochastic gradient descent</b>, allows the learning algorithm to make much faster progress.\n",
    "\n",
    "Repeat { <br>\n",
    "&emsp; &emsp; for $i=1$ to m { <br>\n",
    "$ \\qquad \\qquad \\theta_j \\mathrel{\\mathop:}= \\theta_j - \\alpha (h_\\theta(x^{(i)}) - y^{(i)})\\ x_j^{(i)} \\:\\:\\: \\forall j$ <br>\n",
    "&emsp; &emsp;    } <br>\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__():\n",
    "        self.INIT_PARAMETERS = {\"zero\",\"random\"}\n",
    "        self.OPTIMIZER = {\"gradient_descent\",\"stochastic_gradient_descent\"}\n",
    "        self.theta = None\n",
    "        return\n",
    "    \n",
    "    def fit(X, Y, iterations, init_parameters=\"zero\", optimizer=\"stochastic_gradient_descent\"):\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        nx = X.shape[0]\n",
    "        \n",
    "        n = nx + 1\n",
    "        \n",
    "        # Initialize paramters theta\n",
    "        self._init_weights(n, init_parameters)\n",
    "        return\n",
    "    \n",
    "    def hypothesis_fn():\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict():\n",
    "        return\n",
    "    \n",
    "    def evaluate():\n",
    "        return\n",
    "    \n",
    "    def _init_weights(self, n, init_parameters=\"zero\"):\n",
    "        if init_parameters not in self.INIT_PARAMETERS:\n",
    "            raise ValueError(\"Error: init_parameters must be one of %s.\" % self.INIT_PARAMETERS)\n",
    "            \n",
    "        if init_parameters == \"zero\":\n",
    "            # Initialize paramters with zero values\n",
    "            self.theta = np.zeros((n, 1), dtype=float)\n",
    "            \n",
    "        if init_parameters == \"zero\":\n",
    "            # Initialize paramters with random values\n",
    "            self.theta = np.random.rand(n, 1)\n",
    "        return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
